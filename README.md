# OpenPose training code

## Introduction
This code is buggy and unfinished. It will most probably not work at all!

Based on [Realtime Multi-Person Pose Estimation](https://github.com/ZheC/Realtime_Multi-Person_Pose_Estimation), by [Zhe Cao](http://www.andrew.cmu.edu/user/zhecao), [Tomas Simon](http://www.cs.cmu.edu/~tsimon/), [Shih-En Wei](https://scholar.google.com/citations?user=sFQD3k4AAAAJ&hl=en), [Yaser Sheikh](http://www.cs.cmu.edu/~yaser/).

This project is licensed under the terms of the [license](LICENSE).



## Results
<p align="left">
<img src="https://github.com/ZheC/Multi-Person-Pose-Estimation/blob/master/readme/dance.gif", width="480">
</p>

<p align="left">
<img src="https://github.com/ZheC/Multi-Person-Pose-Estimation/blob/master/readme/shake.gif", width="480">
</p>



## Contents
1. [Testing](#testing)
2. [Training](#training)
3. [Citation](#citation)



## Testing

### C++ (realtime version, for demo purpose)
- Use our modified caffe: [caffe_rtpose](https://github.com/CMU-Perceptual-Computing-Lab/caffe_demo/). Follow the instruction on that repo.
- In May 2017, we released an updated library [OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose).
- Three input options: images, video, webcam.

### Matlab (slower, for COCO evaluation)
- Compatible with general [Caffe](http://caffe.berkeleyvision.org/). Compile matcaffe.
- Run `cd testing; get_model.sh` to retrieve our latest MSCOCO model from our web server.
- Change the caffepath in the `config.m` and run `demo.m` for an example usage.

### Python
- `cd testing/python`.
- `ipython notebook`.
- Open `demo.ipynb` and execute the code.



## Training
### Network Architecture
![Teaser?](https://github.com/ZheC/Multi-Person-Pose-Estimation/blob/master/readme/arch.png)

### Training Steps
- a) Generate data for training:
    - COCO:
        - Option a)
            1. Run `cd training; bash a0_getData.sh` to obtain the COCO images in `dataset/COCO/images/`, keypoints annotations in `dataset/COCO/annotations/` and our custom [COCO official toolbox](https://github.com/gineshidalgo99/cocoapi) in `dataset/COCO/cocoapi/`.
            2. Run `a2_coco_jsonToMat.m` in Matlab to convert the annotation format from json to mat in `dataset/COCO/mat/`.
            3. Run `a3_coco_matToMasks.m` in Matlab to obatin the mask images for unlabeled person. You can use 'parfor' in Matlab to speed up the code.
            4. Run `a4_coco_matToRefinedJson.m` to generate a json file in `dataset/COCO/json/` folder. The json files contain raw informations needed for training.
            5. Run `python a4_genLMDB.py` to generate your COCO and background-COCO LMDBs.
        - Option b)
            - (OUTDATED) Alternatively, you could simply download our prepared LMDB for the COCO dataset (189GB file) by: `bash get_lmdb.sh`. Note, this option no longer works, that COCO LMDB file is no longer compatible.
    - Foot / Face / Hand:
        1. Run `a2_coco_jsonToMat.m` analogously to COCO, but with the foot/face/hand option.
        2. Run `a4_coco_matToRefinedJson.m` analogously to COCO, but with the foot/face/hand option.
        3. Run `python a4_genLMDB.py` again to generate your (COCO+foot)/face/hand LMDB.
    - MPII:
        1. Download [Images (12.9 GB)](https://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1.tar.gz) and [Annotations (12.5 MB)](https://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/mpii_human_pose_v1_u12_2.zip) from the [MPII dataset](http://human-pose.mpi-inf.mpg.de/#download) into `dataset/MPII/`.
        2. Run `a0_convertMatToInitialJson.m`
        3. Run `python a1_generateFinalJsonAndMasks.py` with `sMode = 1` to generate the masks.
        4. Run `python a1_generateFinalJsonAndMasks.py` with `sMode = 2` to generate the final JSON file.
        5. Run `python a4_genLMDB.py` again to generate your MPII LMDB.

- b) Custom Caffe - Download and compile our modified Caffe:
    - Download link: [openpose_caffe_train](https://github.com/gineshidalgo99/openpose_caffe_train).
    - Compile it by running: `make all -j{num_cores} && make pycaffe -j{num_cores}`.
    - Note: It will be merged with caffe_rtpose (for testing) soon.

- c) Generate Caffe ProtoTxt and shell file for training:
    - Run `python d_setLayers.py`.

- d) Pre-trained model (VGG-19) - Get pre-trained model to initialize the first 10 layers of the network for training:
    - Download link: [VGG-19 model](https://gist.github.com/ksimonyan/3785162f95cd2d5fee77).

- e) Train:
    - Run `bash train_pose.sh 0,1,2,3` (generated by setLayers.py) to start the training with 4 GPUs (0-3).



## Related Repositories
- Our new C++ library [OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose).
- CVPR'16, [Convolutional Pose Machines](https://github.com/shihenw/convolutional-pose-machines-release).
- [Pytorch version of the code](https://github.com/tensorboy/pytorch_Realtime_Multi-Person_Pose_Estimation).



## Citation
Please cite the paper in your publications if it helps your research:

    @inproceedings{cao2017realtime,
      author = {Zhe Cao and Tomas Simon and Shih-En Wei and Yaser Sheikh},
      booktitle = {CVPR},
      title = {Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},
      year = {2017}
      }

    @inproceedings{wei2016cpm,
      author = {Shih-En Wei and Varun Ramakrishna and Takeo Kanade and Yaser Sheikh},
      booktitle = {CVPR},
      title = {Convolutional pose machines},
      year = {2016}
      }
